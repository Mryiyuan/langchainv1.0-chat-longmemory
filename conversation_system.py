import sqlite3
import uuid
from typing import TypedDict, List

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.documents import Document
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver  # â† åŠ è¿™ä¸ª

from embeddings_model import get_embeddings
from config import config, milvus_config

from langchain_openai import ChatOpenAI
from langchain_milvus import Milvus


# ============================================
# å®šä¹‰çŠ¶æ€ï¼ˆç»™ LangGraph ç”¨ï¼‰
# ============================================
class ConversationState(TypedDict):
    messages: List[BaseMessage]


# ============================================
# åˆå§‹åŒ– SQLite æ•°æ®åº“
# ============================================
def init_db():
    conn = sqlite3.connect("conversations.db")
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY,
            thread_id TEXT,
            role TEXT,
            content TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    conn.close()


# ============================================
# åˆå§‹åŒ– LLM
# ============================================
def init_llm():
    return ChatOpenAI(
        model=config.deepseek_REASONER_MODEL_NAME,
        api_key=config.deepseek_API_KEY,
        base_url=config.deepseek_BASE_URL,
        temperature=0.7,
    )


# ============================================
# åˆå§‹åŒ– Milvus
# ============================================
def init_vector_store():
    embeddings = get_embeddings()
    return Milvus(
        embedding_function=embeddings,
        connection_args={"uri": milvus_config.MILVUS_URI},
        collection_name="user_memories",
        index_params={"index_type": "HNSW", "metric_type": "L2"},
        auto_id=True,
    )


# ============================================
# SQLite æ“ä½œ
# ============================================
def save_message(thread_id: str, role: str, content: str):
    """ä¿å­˜æ¶ˆæ¯åˆ° SQLite"""
    try:
        conn = sqlite3.connect("conversations.db")
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO messages (thread_id, role, content) VALUES (?, ?, ?)",
            (thread_id, role, content)
        )
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")


def load_messages(thread_id: str) -> List[BaseMessage]:
    """ä» SQLite åŠ è½½æ¶ˆæ¯"""
    try:
        conn = sqlite3.connect("conversations.db")
        cursor = conn.cursor()
        cursor.execute(
            "SELECT role, content FROM messages WHERE thread_id = ? ORDER BY id",
            (thread_id,)
        )
        rows = cursor.fetchall()
        conn.close()
        
        messages = []
        for role, content in rows:
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))
        
        return messages
    except Exception as e:
        print(f"âŒ åŠ è½½æ¶ˆæ¯å¤±è´¥: {e}")
        return []


def get_message_count(thread_id: str) -> int:
    """è·å–æŸä¸ªçº¿ç¨‹çš„æ¶ˆæ¯æ•°"""
    try:
        conn = sqlite3.connect("conversations.db")
        cursor = conn.cursor()
        cursor.execute(
            "SELECT COUNT(*) FROM messages WHERE thread_id = ?",
            (thread_id,)
        )
        count = cursor.fetchone()[0]
        conn.close()
        return count
    except:
        return 0


# ============================================
# é•¿æœŸè®°å¿†æ£€ç´¢
# ============================================
def retrieve_memory(query: str, vector_store: Milvus) -> str:
    try:
        docs = vector_store.similarity_search(query, k=3)
        if not docs:
            return ""
        return "\n".join(f"- {d.page_content}" for d in docs)
    except:
        return ""


# ============================================
# LLM èŠ‚ç‚¹ï¼ˆç»™ LangGraph ç”¨ï¼‰
# ============================================
def llm_node(state: ConversationState, vector_store) -> dict:
    """LLM å¤„ç†èŠ‚ç‚¹"""
    llm = init_llm()
    
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_msg = state["messages"][-1].content
    
    # ä»é•¿æœŸè®°å¿†æ£€ç´¢
    long_term_memory = retrieve_memory(last_user_msg, vector_store)

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

=== é•¿æœŸè®°å¿†ï¼ˆè·¨çº¿ç¨‹ï¼‰ ===
{long_term_memory if long_term_memory else "ï¼ˆæš‚æ— ç›¸å…³è®°å¿†ï¼‰"}

=== å¯¹è¯æŒ‡å¼• ===
- åŸºäºå¯¹è¯å†å²å’Œé•¿æœŸè®°å¿†å›ç­”
- è¯†åˆ«å¹¶è®°ä½ç”¨æˆ·çš„é‡è¦ä¿¡æ¯
- ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„è¯­æ€"""

    # åªç”¨æœ€è¿‘ 6 æ¡æ¶ˆæ¯
    short_window = state["messages"][-6:]

    response = llm.invoke(
        [SystemMessage(content=system_prompt)] + short_window
    )

    return {"messages": [response]}


# ============================================
# æ„å»º LangGraphï¼ˆç”¨äº MemorySaverï¼‰
# ============================================
def build_graph(vector_store):
    """æ„å»ºå¯¹è¯å›¾ï¼Œä½¿ç”¨ MemorySaver åšçŸ­æœŸè®°å¿†"""
    builder = StateGraph(ConversationState)
    
    builder.add_node("llm", lambda s: llm_node(s, vector_store))
    builder.add_edge(START, "llm")
    builder.add_edge("llm", END)
    
    # âœ… ç”¨ MemorySaver åšçŸ­æœŸè®°å¿†ï¼ˆç¨‹åºè¿è¡ŒæœŸé—´ï¼‰
    checkpointer = MemorySaver()
    graph = builder.compile(checkpointer=checkpointer)
    
    return graph


# ============================================
# æ˜¯å¦éœ€è¦ä¿å­˜é•¿æœŸè®°å¿†ï¼ˆç”¨ LLM æ™ºèƒ½åˆ¤æ–­ï¼‰
# ============================================
def should_store_memory(text: str, vector_store) -> bool:
    """ç”¨ LLM åˆ¤æ–­æ˜¯å¦åŒ…å«é‡è¦ä¸ªäººä¿¡æ¯"""
    llm = init_llm()
    
    judge_prompt = f"""åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åŒ…å«é‡è¦çš„ä¸ªäººä¿¡æ¯ï¼ˆåå­—ã€å·¥ä½œã€çˆ±å¥½ã€ç»å†ç­‰ï¼‰ã€‚
åªæœ‰åŒ…å«"ç”¨æˆ·æœ¬äººçš„é‡è¦ä¿¡æ¯"æ‰è¿”å›"æ˜¯"ã€‚ä¸è¦ä¿å­˜å…³äºå¤©æ°”ã€é£Ÿç‰©ç­‰ä¸€èˆ¬æ€§è¯é¢˜ã€‚

ç”¨æˆ·æ¶ˆæ¯: {text}

è¯·å›ç­”"æ˜¯"æˆ–"å¦"ï¼Œåªè¿”å›ä¸€ä¸ªå­—ï¼š"""

    try:
        response = llm.invoke(judge_prompt).content.strip()
        return "æ˜¯" in response
    except:
        return False


def extract_memory(text: str, vector_store) -> str:
    """ç”¨ LLM æå–é‡è¦ä¿¡æ¯"""
    llm = init_llm()
    
    extract_prompt = f"""ä»ä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯ä¸­æå–é‡è¦çš„ä¸ªäººä¿¡æ¯ã€‚
å¦‚æœæ²¡æœ‰é‡è¦ä¿¡æ¯ï¼Œè¿”å›"æ— "ã€‚

ç”¨æˆ·æ¶ˆæ¯: {text}

è¯·æå–ä¿¡æ¯ï¼ˆæ ¼å¼è‡ªç”±ï¼‰æˆ–è¿”å›"æ— "ï¼š"""

    try:
        response = llm.invoke(extract_prompt).content.strip()
        if response != "æ— ":
            return response
        return None
    except:
        return None


# ============================================
# ConversationManager
# ============================================
class ConversationManager:

    def __init__(self):
        init_db()
        self.vector_store = init_vector_store()
        # âœ… ç”¨ LangGraph + MemorySaver åšçŸ­æœŸè®°å¿†
        self.graph = build_graph(self.vector_store)

    def create_thread(self) -> str:
        return str(uuid.uuid4())

    def stream_message(self, thread_id: str, text: str):
        """æµå¼è¾“å‡ºæ¶ˆæ¯"""
        config = {"configurable": {"thread_id": thread_id}}
        
        # âœ… æ–¹æ¡ˆ 1ï¼šå…ˆä» SQLite åŠ è½½å†å²
        previous_messages = load_messages(thread_id)
        
        # åˆå¹¶å†å² + æ–°æ¶ˆæ¯
        all_messages = previous_messages + [HumanMessage(content=text)]
        
        # âœ… æ–¹æ¡ˆ 2ï¼šç”¨ LangGraph + MemorySaver å¤„ç†å½“å‰ä¼šè¯
        # ï¼ˆLangGraph ä¼šè‡ªåŠ¨ç”¨ MemorySaver ä¿å­˜çŠ¶æ€åœ¨å†…å­˜ä¸­ï¼‰
        full_response = ""
        for event in self.graph.stream(
            {"messages": all_messages},
            config,
            stream_mode="values"
        ):
            if "messages" in event:
                msg = event["messages"][-1]
                if isinstance(msg, AIMessage):
                    # æµå¼è¾“å‡º
                    for chunk in msg.content:
                        yield chunk
                        full_response += chunk
        
        # âœ… æ–¹æ¡ˆ 3ï¼šä¿å­˜åˆ° SQLiteï¼ˆæŒä¹…åŒ–ï¼‰
        save_message(thread_id, "user", text)
        save_message(thread_id, "assistant", full_response)
        
        # âœ… æ–¹æ¡ˆ 4ï¼šä¿å­˜é•¿æœŸè®°å¿†åˆ° Milvusï¼ˆè·¨çº¿ç¨‹ï¼‰
        if should_store_memory(text, self.vector_store):
            memory = extract_memory(text, self.vector_store)
            if memory:
                try:
                    self.vector_store.add_documents([
                        Document(page_content=memory)
                    ])
                    print(f"ğŸ’¾ å·²ä¿å­˜é•¿æœŸè®°å¿†: {memory[:50]}...")
                except:
                    pass

    def get_history(self, thread_id: str) -> list:
        """è·å–å¯¹è¯å†å²"""
        messages = load_messages(thread_id)
        
        result = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                result.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                result.append({"role": "assistant", "content": msg.content})
        
        return result

    def get_long_term_memory(self) -> list:
        """è·å–æ‰€æœ‰é•¿æœŸè®°å¿†"""
        try:
            docs = self.vector_store.similarity_search("ç”¨æˆ·", k=20)
            return [d.page_content for d in docs]
        except:
            return []
    
    def get_thread_stats(self, thread_id: str) -> dict:
        """è·å–å¯¹è¯ç»Ÿè®¡"""
        messages = load_messages(thread_id)
        user_count = len([m for m in messages if isinstance(m, HumanMessage)])
        ai_count = len([m for m in messages if isinstance(m, AIMessage)])
        
        return {
            "total_messages": len(messages),
            "user_messages": user_count,
            "ai_messages": ai_count,
            "turns": user_count
        }